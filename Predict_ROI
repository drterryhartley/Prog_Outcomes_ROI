import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error
from scipy.stats import shapiro
from statsmodels.stats.outliers_influence import variance_inflation_factor
import statsmodels.api as sm

# 1. Check for linearity using pair plots
def check_linearity(data, target_col):
    sns.pairplot(data)
    plt.show()

# 2. Check for multicollinearity using VIF (Variance Inflation Factor)
def check_multicollinearity(X):
    vif_data = pd.DataFrame()
    vif_data["Feature"] = X.columns
    vif_data["VIF"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]
    return vif_data

# 3. Check for homoscedasticity by plotting residuals vs. fitted values
def check_homoscedasticity(y_true, y_pred):
    residuals = y_true - y_pred
    plt.scatter(y_pred, residuals)
    plt.axhline(y=0, color='r', linestyle='--')
    plt.title("Residuals vs Fitted")
    plt.xlabel("Fitted values")
    plt.ylabel("Residuals")
    plt.show()

# 4. Check for normality of residuals using a Q-Q plot and Shapiro-Wilk test
def check_normality_of_residuals(residuals):
    sm.qqplot(residuals, line ='45')
    plt.title("Q-Q Plot")
    plt.show()

    # Shapiro-Wilk Test
    stat, p_value = shapiro(residuals)
    print(f'Shapiro-Wilk Test p-value: {p_value}')
    if p_value > 0.05:
        print("Residuals are normally distributed (Fail to reject null hypothesis).")
    else:
        print("Residuals are not normally distributed (Reject null hypothesis).")

# Generate sample data (same as before)
np.random.seed(42)

provider_quality_outcomes = np.random.uniform(50, 100, 100)
membership_trending = np.random.uniform(-10, 10, 100)
program_expenditures = np.random.uniform(50000, 500000, 100)
time_between_intake_contact = np.random.uniform(1, 30, 100)
sessions_per_member_per_diagnosis = np.random.uniform(1, 10, 100)
referrals_in = np.random.randint(1, 20, 100)
referrals_out = np.random.randint(1, 20, 100)
timely_appointments = np.random.uniform(0, 100, 100)
provider_communication = np.random.uniform(0, 100, 100)
shared_decision_making = np.random.uniform(0, 100, 100)
customer_service = np.random.uniform(0, 100, 100)
overall_provider_rating = np.random.uniform(0, 100, 100)

predicted_savings = (
    provider_quality_outcomes * 0.4
    + membership_trending * 150
    + sessions_per_member_per_diagnosis * 50
    + overall_provider_rating * 0.5
    - program_expenditures * 0.0001
)

data = pd.DataFrame({
    'Provider_Quality_Outcomes': provider_quality_outcomes,
    'Membership_Trending': membership_trending,
    'Program_Expenditures': program_expenditures,
    'Time_Between_Intake_And_Contact': time_between_intake_contact,
    'Sessions_Per_Member_Per_Diagnosis': sessions_per_member_per_diagnosis,
    'Referrals_In': referrals_in,
    'Referrals_Out': referrals_out,
    'Timely_Appointments_Score': timely_appointments,
    'Provider_Communication_Score': provider_communication,
    'Shared_Decision_Making_Score': shared_decision_making,
    'Customer_Service_Score': customer_service,
    'Overall_Provider_Rating': overall_provider_rating,
    'Predicted_Savings': predicted_savings
})

# Define features (X) and target (y)
X = data[[
    'Provider_Quality_Outcomes',
    'Membership_Trending',
    'Program_Expenditures',
    'Time_Between_Intake_And_Contact',
    'Sessions_Per_Member_Per_Diagnosis',
    'Referrals_In',
    'Referrals_Out',
    'Timely_Appointments_Score',
    'Provider_Communication_Score',
    'Shared_Decision_Making_Score',
    'Customer_Service_Score',
    'Overall_Provider_Rating'
]]
y = data['Predicted_Savings']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 1. Check Linearity (Visual inspection)
print("Checking Linearity...")
check_linearity(X, 'Predicted_Savings')

# 2. Check Multicollinearity (VIF should be less than 10 for all variables)
print("\nChecking Multicollinearity...")
vif_data = check_multicollinearity(X_train)
print(vif_data)

# 3. Fit the model
model = LinearRegression()
model.fit(X_train, y_train)

# Predict savings (ROI) on the test set
y_pred = model.predict(X_test)

# 4. Check Homoscedasticity
print("\nChecking Homoscedasticity...")
check_homoscedasticity(y_test, y_pred)

# 5. Check Normality of Residuals
print("\nChecking Normality of Residuals...")
residuals = y_test - y_pred
check_normality_of_residuals(residuals)

# Evaluate the model
mae = mean_absolute_error(y_test, y_pred)
print(f'\nMean Absolute Error: {mae}')

# Compare the predicted savings with actual program expenditures
comparison = pd.DataFrame({
    'Predicted_Savings': y_pred,
    'Program_Expenditures': X_test['Program_Expenditures'],
})

# Add a comparison column to show the difference between savings and expenditures
comparison['Savings_vs_Expenditures'] = comparison['Predicted_Savings'] - comparison['Program_Expenditures']

# Generate a report by summarizing the comparison
report = comparison.describe()

# Plot the predicted savings vs. actual expenditures for visualization
plt.figure(figsize=(10, 6))
plt.scatter(comparison['Program_Expenditures'], comparison['Predicted_Savings'], color='blue')
plt.plot([0, 500000], [0, 500000], color='red', linestyle='--')
plt.title('Predicted Savings (ROI) vs Program Expenditures')
plt.xlabel('Program Expenditures')
plt.ylabel('Predicted Savings (ROI)')
plt.grid(True)
plt.show()

# Show the summary report
print("\nSummary Report:")
print(report)
